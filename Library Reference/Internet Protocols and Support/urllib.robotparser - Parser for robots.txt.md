# urllib.robotparser - Parser for robots.txt
> GitHub@[orca-j35](https://github.com/orca-j35)ï¼Œæ‰€æœ‰ç¬”è®°å‡æ‰˜ç®¡äºŽ [python_notes](https://github.com/orca-j35/python_notes) ä»“åº“
>
> å‚è€ƒ:
>
> - [`urllib.robotparser`](https://docs.python.org/3/library/urllib.robotparser.html#module-urllib.robotparser) â€” Parser for robots.txt
> - [urllib.robotparser â€” Internet Spider Access Control](https://pymotw.com/3/urllib.robotparser/index.html)
> - [13.4. urllib.robotparser â€” äº’è”ç½‘çˆ¬è™«è®¿é—®æŽ§åˆ¶](https://learnku.com/docs/pymotw/urllibrobotparser-internet-spider-access-control/3434)

è¯¥æ¨¡å—ä»…æä¾›äº†ä¸€ä¸ªç”¨äºŽåˆ†æž `robots.txt` æ–‡ä»¶çš„ `RobotFileParser` çš„ç±»ã€‚

æœ‰å…³ `robots.txt` æ–‡ä»¶ç»“æž„çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒ: http://www.robotstxt.org/orig.html

## RobotFileParser

ðŸ”¨class urllib.robotparser.RobotFileParser(*url*='')

This class provides methods to read, parse and answer questions about the robots.txt file at url.

- set_url(*url*)
  Sets the URL referring to a robots.txt file.
- read()
  Reads the robots.txt URL and feeds it to the parser.
- parse(*lines*)
  Parses the lines argument.

can_fetch(*useragent*, *url*)
Returns True if the useragent is allowed to fetch the url according to the rules contained in the parsed robots.txt file.

mtime()
Returns the time the robots.txt file was last fetched. This is useful for long-running web spiders that need to check for new robots.txt files periodically.

modified()
Sets the time the robots.txt file was last fetched to the current time.

crawl_delay(*useragent*)
Returns the value of the Crawl-delay parameter from robots.txt for the useragent in question. If there is no such parameter or it doesnâ€™t apply to the useragent specified or the robots.txt entry for this parameter has invalid syntax, return None.

*New in version 3.6.*

request_rate(*useragent*)
Returns the contents of the Request-rate parameter from robots.txt as a named tuple RequestRate(requests, seconds). If there is no such parameter or it doesnâ€™t apply to the useragent specified or the robots.txt entry for this parameter has invalid syntax, return None.

*New in version 3.6.*

The following example demonstrates basic use of the [`RobotFileParser`](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser) class:

```python
>>> import urllib.robotparser
>>> rp = urllib.robotparser.RobotFileParser()
>>> rp.set_url("http://www.musi-cal.com/robots.txt")
>>> rp.read()
>>> rrate = rp.request_rate("*")
>>> rrate.requests
3
>>> rrate.seconds
20
>>> rp.crawl_delay("*")
6
>>> rp.can_fetch("*", "http://www.musi-cal.com/cgi-bin/search?city=San+Francisco")
False
>>> rp.can_fetch("*", "http://www.musi-cal.com/")
True
```

