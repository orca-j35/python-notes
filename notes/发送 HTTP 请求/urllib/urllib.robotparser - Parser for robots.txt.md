# urllib.robotparser - Parser for robots.txt
> GitHub@[orca-j35](https://github.com/orca-j35)ï¼Œæ‰€æœ‰ç¬”è®°å‡æ‰˜ç®¡äºŽ [python_notes](https://github.com/orca-j35/python_notes) ä»“åº“
>
> å‚è€ƒ:
>
> - [`urllib.robotparser`](https://docs.python.org/3/library/urllib.robotparser.html#module-urllib.robotparser) â€” Parser for robots.txt
> - [urllib.robotparser â€” Internet Spider Access Control](https://pymotw.com/3/urllib.robotparser/index.html)
> - [13.4. urllib.robotparser â€” äº’è”ç½‘çˆ¬è™«è®¿é—®æŽ§åˆ¶](https://learnku.com/docs/pymotw/urllibrobotparser-internet-spider-access-control/3434)
> - https://en.wikipedia.org/wiki/Robots_exclusion_standard

è¯¥æ¨¡å—æä¾›äº†ä¸€ä¸ªç”¨äºŽåˆ†æž [`robots.txt`](https://en.wikipedia.org/wiki/Robots_exclusion_standard) æ–‡ä»¶çš„ `RobotFileParser` çš„ç±»ã€‚

## robots.txt

`robots.txt` æ˜¯ä¸€ç§åŸºäºŽæ–‡æœ¬çš„è®¿é—®æŽ§åˆ¶ç³»ç»Ÿï¼ŒWeb ç«™ç‚¹å¯åˆ©ç”¨æ ¹ç›®å½•ä¸‹çš„ `robots.txt` æ–‡ä»¶æ¥çº¦å®šå¯è¢«çˆ¬å–çš„èŒƒå›´ï¼Œä¾‹å¦‚:

```
# robots.txt
User-agent: *
Allow: /public/
Disallow: /
```

ä¸Šé¢è¿™ä¸ª `/robots.txt` åªå…è®¸çˆ¬è™«çˆ¬å– `/public/` ç›®å½•ä¸­çš„å†…å®¹ã€‚`Allow` éœ€å’Œ `Disallow ` ä¸€èµ·ä½¿ç”¨ï¼Œå¹¶ä½äºŽ `Disallow ` å‰é¢ï¼Œè¡¨ç¤ºå…è®¸è®¿é—® `Disallow` ä¸­çš„æŸäº›ç›®å½•ã€‚

`User-agent: *` è¡¨ç¤ºå¯¹æ‰€æœ‰çˆ¬è™«ç¨‹åºæœ‰æ•ˆï¼Œå¦‚æžœè®¾ç½®ä¸º `User-agent: Baiduspider`ï¼Œåˆ™è¡¨ç¤ºæ‰€è®¾ç½®çš„è§„åˆ™ä»…å¯¹ç™¾åº¦çˆ¬è™«æœ‰æ•ˆã€‚å¤§å…¬å¸çš„çˆ¬è™«éƒ½æœ‰å›ºå®šçš„ user-agent æ ‡è¯†ç¬¦ï¼Œä¾‹å¦‚:

| user-agent  | companies         |
| ----------- | ----------------- |
| BaiduSpider | www.baidu.com     |
| Googlebot   | www.google.com    |
| 360Spider   | www.so.com        |
| YodaoBot    | www.youdao.com    |
| ia_archiver | www.alexa.cn      |
| Scooter     | www.altavista.com |

æœ‰å…³ `robots.txt` ç»“æž„çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒ: 

- http://www.robotstxt.org/orig.html
- http://www.robotstxt.org/robotstxt.html
- https://en.wikipedia.org/wiki/Robots_exclusion_standard

## RobotFileParser

ðŸ”¨class urllib.robotparser.RobotFileParser(*url*='')

è¯¥ç±»æä¾›è¯»å–å’Œè§£æž `/robots.txt` æ–‡ä»¶çš„æ–¹æ³•ã€‚

- set_url(*url*) - ç”¨äºŽè®¾ç½® `robots.txt` çš„ URLï¼Œå¦‚æžœåœ¨åˆ›å»º `RobotFileParser` å®žä¾‹æ—¶æ²¡æœ‰ä¼ å…¥ URLï¼Œåˆ™éœ€è°ƒç”¨ `set_url()` è¿›è¡Œè®¾ç½®ã€‚

- read() - è¯»å– `robots.txt` çš„ URL å¹¶å°†å…¶æä¾›ç»™è§£æžå™¨ `parse()`ã€‚

- parse(*lines*) - è§£æž *lines* åˆ—è¡¨ä¸­çš„ robots è§„åˆ™

  ```python
  '''https://pymotw.com/robots.txt
  Sitemap: https://pymotw.com/sitemap.xml
  User-agent: *
  Disallow: /admin/
  Disallow: /downloads/
  Disallow: /media/
  Disallow: /static/
  Disallow: /codehosting/
  '''
  from urllib import request
  from urllib import robotparser
  
  parser = robotparser.RobotFileParser()
  parser.parse(request.urlopen('https://pymotw.com/robots.txt').read().decode('utf-8').splitlines())
  print(parser.can_fetch('*', 'https://pymotw.com/media/'))
  #> False
  print(parser.can_fetch('*', 'https://pymotw.com/3/urllib.robotparser'))
  #> True
  ```

- can_fetch(*useragent*, *url*) - æ ¹æ® `robots.txt` ä¸­è®¾å®šçš„è§„åˆ™æ¥åˆ¤æ–­ *useragent* æ˜¯å¦èƒ½å¤ŸæŠ“å– *url*ã€‚åœ¨è°ƒç”¨ `can_fetch()` å‰éœ€è¦å…ˆè°ƒç”¨ `read()` æˆ– `parse()` æ¥è§£æž `robots.txt` ä¸­çš„å†…å®¹ï¼Œå¦åˆ™  `can_fetch()` å§‹ç»ˆä¼šè¿”å›ž `False`ã€‚

- mtime() - è¿”å›žä¸Šæ¬¡èŽ·å– `robots.txt` çš„æ—¶é—´ã€‚è¿™å¯¹äºŽéœ€è¦å®šæœŸæ£€æŸ¥ `robots.txt` çš„ Web çˆ¬è™«éžå¸¸æœ‰ç”¨ã€‚

- modified() - å°†ä¸Šä¸€æ¬¡æŠ“å– `robots.txt` çš„æ—¶é—´è®¾ç½®ä¸ºå½“å‰æ—¶é—´

- crawl_delay(*useragent*) - è¿”å›ž `robts.txt` ä¸­ä¸º *useragent* é…ç½® `Crawl-delay` å‚æ•°ã€‚å¦‚æžœå‡ºçŽ°ä»¥ä¸‹æƒ…å†µï¼Œå°†è¿”å›ž `None`:

  - æ²¡æœ‰åœ¨ `robots.txt` ä¸­é…ç½® `Crawl-delay` å‚æ•°
  - `Crawl-delay` å‚æ•°ä¸é€‚ç”¨äºŽ *useragent*
  - `Crawl-delay` å…·å¤‡æ— æ•ˆè¯­æ³•ã€‚

  *New in version 3.6.*

  Hint: å¯åˆ©ç”¨ `robots.txt` ä¸­ `Crawl-delay` è§„åˆ™æ¥çº¦å®šçˆ¬è™«çš„æŠ“å–å»¶è¿Ÿï¼Œä¾‹å¦‚:

  ```python
  User-agent: *
  Crawl-delay: 10
  ```

  è¯¦è§: https://en.wikipedia.org/wiki/Robots_exclusion_standard#Crawl-delay_directive

- request_rate(*useragent*) - èŽ·å– `robts.txt` ä¸­ä¸º *useragent* é…ç½® `Request-rate` å‚æ•°ï¼Œè¿”å›žå€¼æ˜¯  [named tuple](https://docs.python.org/3/glossary.html#term-named-tuple) `RequestRate(requests, seconds)`ã€‚å¦‚æžœå‡ºçŽ°ä»¥ä¸‹æƒ…å†µï¼Œå°†è¿”å›ž `None`:

  - æ²¡æœ‰åœ¨ `robots.txt` ä¸­é…ç½® `Request-rate` å‚æ•°
  - `Request-rate` å‚æ•°ä¸é€‚ç”¨äºŽ *useragent*
  - `Request-rate` å…·å¤‡æ— æ•ˆè¯­æ³•ã€‚

  *New in version 3.6.*

## Example

ç¤ºä¾‹ - æ¼”ç¤ºäº† [`RobotFileParser`](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser) ç±»çš„åŸºæœ¬ä½¿ç”¨æ–¹æ³•:

```python
>>> import urllib.robotparser
>>> rp = urllib.robotparser.RobotFileParser()
>>> rp.set_url("http://www.musi-cal.com/robots.txt")
>>> rp.read()
>>> rrate = rp.request_rate("*")
>>> rrate.requests
3
>>> rrate.seconds
20
>>> rp.crawl_delay("*")
6
>>> rp.can_fetch("*", "http://www.musi-cal.com/cgi-bin/search?city=San+Francisco")
False
>>> rp.can_fetch("*", "http://www.musi-cal.com/")
True
```

ç¤ºä¾‹ - åˆ©ç”¨ `mtime()` æ–¹æ³•å®šæœŸæ£€æŸ¥ `robots.txt` çš„æ›´æ–°:

```python
from urllib import robotparser
from urllib import parse
import time

AGENT_NAME = 'PyMOTW'
URL_BASE = 'https://pymotw.com/'
parser = robotparser.RobotFileParser()
parser.set_url(parse.urljoin(URL_BASE, 'robots.txt'))
parser.read()
parser.modified()

PATHS = [
    '/',
    '/PyMOTW/',
    '/admin/',
    '/downloads/PyMOTW-1.92.tar.gz',
]

for path in PATHS:
    age = int(time.time() - parser.mtime())
    print('age:', age, end=' ')
    if age > 1:
        print('rereading robots.txt')
        parser.read()
        parser.modified()
    else:
        print()
    print('{!r:>6} : {}'.format(parser.can_fetch(AGENT_NAME, path), path))
    # Simulate a delay in processing
    time.sleep(1)
    print('.')
'''Out:
age: 0
  True : /
.
age: 1
  True : /PyMOTW/
.
age: 2 rereading robots.txt
 False : /admin/
.
age: 1
 False : /downloads/PyMOTW-1.92.tar.gz
.
'''
```

